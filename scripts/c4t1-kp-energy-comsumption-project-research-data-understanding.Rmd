---
title: "c4t1-kp-energy-consumption-initial"
author: "Katherine Piatti"
date: "3/25/2021"
output:
   html_document:
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# PROJECT SUMMARY #########################################

The goal of this project is to analyze energy consumption data to determine if there's evidence supporting a client's claim that they were not occupying the home during the summer of 2008 (June-August).

Questions from the client:
 
  1. What information is contained within the data records?
  2. Is the data complete? Is anything missing?
  3. What are the typical power usage patterns for this residence?
  4. Are these typical patterns true for the summer 2008? If not, what, if anything, can be used to help support the client’s claims?
  5. Are there any outliers or events in the data that may undermine the client’s claims?
  6. Are there any recommendations for questions we should be asking?
  7. Do you need anyy additional information to conduct the analysis?
  



# DATA #################################


Dataset Information (from documentation)

  - contains 2,075,259 measurements 
  - gathered in a house located in Sceaux (7km of Paris, France)
  - gathered between December 2006 and November 2010 (47 months)
  
  Notes:
   
   - (global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) by electrical equipment not measured in sub-meterings 1, 2 and 3.
   - The dataset contains some missing measurements (~ 1.25% of rows). All calendar timestamps are present but measurement values are missing for some: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators (e.g. on April 28, 2007).

**Attribute Information**

  1. date: Date in format dd/mm/yyyy
  2. time: time in format hh:mm:ss
  3. global_active_power: household global minute-averaged active power (in kilowatt)
  4. global_reactive_power: household global minute-averaged reactive power (in kilowatt)
  5. voltage: minute-averaged voltage (in volt)
  6. global_intensity: household global minute-averaged current intensity (in ampere)
  sub meter variables: energy sub-metering (in watt-hour of active energy).     
  7. sub_metering_1: corresponds to the kitchen (dishwasher, oven and microwave)
  8. sub_metering_2: corresponds to the laundry room (contains a washing-machine, tumble-drier, refrigerator and a light)
  9. sub_metering_3: corresponds to an electric water-heater and an air-conditioner.



### Sub-meters

Sub-meters provide room-by-room and/or appliance specific data about energy usage. In other words, a building/residence has a main/master meter that records the energy usage for the entire building/residence. Readings from main meters are what energy companies use for billing purposes. In addition to main meters, buildings/residences may also have sub-meters that just record energy usage from particular areas/rooms or applicances. Building managers and consumers typically use sub-meter data to get insight into and, perhaps improve, energy usage in particular areas.




## LOAD PKGS & GET DATA #################################

```{r, warning=FALSE, message=FALSE}
#load pkgs 
library(DBI) # for connecting to MySQL
library(tidyverse)
# tidyverse wouldn't install when typing in mannually, ran install.packages() in console then selected tidyverse from popup menu
library(janitor)

library(gghighlight)

# pkgs for working with time series data
library(lubridate) # wrangling date and time data
library(tsibble) # time series data representation (tibble-like)
library(feasts) # time series visualization
library(fable) # time series forecasting
```


## DB Connection
```{r}
# create connection to MySQL database
con <-  dbConnect(RMariaDB::MariaDB(),
                user='deepAnalytics',
                password='Sqltask1234!',
                dbname='dataanalytics2018',
                host='data-analytics-2018.cbrosir2cswx.us-east-1.rds.amazonaws.com')
```

>*Reminder*: call dbDisconnect() when finished working with a connection

## Get Energy Data

```{r}
# list tables contained in database
dbListTables(con)
```

*2006*
```{r}
# list vars in yr_2006 table
dbListFields(con, 'yr_2006')
```

```{r}
# import select vars from yr_2006
yr2006 <- dbGetQuery(con, 'SELECT Date, time, sub_metering_1, sub_metering_2, sub_metering_3 FROM yr_2006')

#check import
glimpse(yr2006)
```

*2007*
```{r}
# list vars in yr_2007 table
dbListFields(con, 'yr_2007')
```

```{r}
# import select vars from yr 2007
yr2007 <- dbGetQuery(con, 'SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2007')

#check import
glimpse(yr2007)

# since all showing submeter values are 0, perform additional check
unique(yr2007$Sub_metering_3)
```

*2008*
```{r}
# list var names for yr_2008
dbListFields(con, 'yr_2008')
```

```{r}
# import select vars from yr 2008
yr2008 <- dbGetQuery(con, 'SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2008')

#check import
glimpse(yr2008)
```

*2009*
```{r}
# list vars in yr 2009
dbListFields(con, 'yr_2009')
```

```{r}
# import select vars from yr 2009
yr2009 <- dbGetQuery(con, 'SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2009')

# check import
glimpse(yr2009)

#perform 2nd check b/c all showing values are 0
unique(yr2009$Sub_metering_3)
```

*2010*
```{r}
# list vars in yr 2010
dbListFields(con, 'yr_2010')
```

```{r}
# import select vars from yr_2010
yr2010 <- dbGetQuery(con, 'SELECT Date, Time, Sub_metering_1, Sub_metering_2, Sub_metering_3 FROM yr_2010')

# check import
glimpse(yr2010)
```




# DATA WRANGLING ################################

## Find Complete Datasets

I only want to include datasets that have submeter readings for the full year. I'll check the unique values in "Date" column to determine which years are complete.  

*2006*
```{r}
#get unique values in date column
unique(yr2006$Date)
```
2006 data only covers part of December. 


*2007*
```{r}
# get unique values in date
unique(yr2007$Date)
```
2007 data covers full year.


*2008*
```{r}
# get unique values in date
unique(yr2008$Date)
```
2008 data covers full year. 


*2009*
```{r}
# get unique values in date
unique(yr2009$Date)
```
2009 data covers full year. 


*2010*
```{r}
# get unique values in date
unique(yr2010$Date)
```
2010 data only covers through November 26.



## Join 2007-09 Datasets 

```{r, results='hide'}
# make var names uniform
yr2007 <- clean_names(yr2007)
yr2008 <- clean_names(yr2008)
yr2009 <- clean_names(yr2009)



# merge 2007-2009 datasets
yrs2007_2009 <- bind_rows(yr2007, yr2008, yr2009)
```




## Rename Variables

```{r}
yrs2007_2009 <- yrs2007_2009 %>% 
  rename(kitchen = sub_metering_1) %>% 
  rename(laundry = sub_metering_2) %>% 
  rename(AC = sub_metering_3)

glimpse(yrs2007_2009)
```




## Create Date-Time Variable

*Unite Columns*
```{r}
# create new column uniting date and time, converr to posixct
yrs2007_2009 <- yrs2007_2009 %>% 
  mutate(date_time = paste(yrs2007_2009$date,
                           yrs2007_2009$time)) %>% 
  mutate(date_time = ymd_hms(date_time)) 

glimpse(yrs2007_2009)
```

>*Notes on Adding Time Zone*
>
- POA says to add Paris timezone, but I don't think including time zone would add anything to this analysis (and working with time zoned values is much harder)
  + To add Paris time zone use `mutate(date_time = force_tz(date_time, tzone = "Europe/Paris"))`
- Use `OlsonNames()` to get list of timezones on your machine.
- `force_tz()` will add time zone to date_time object without changing the clock time and altering the times in your data.
- `with_tz()` changes the time zone and updates the clock time to align with the updated time zone--this may alter the time values in your data.



## Create Tsibble

```{r}
# remove columns, reshape from wide to long, coerce tsibble
yrs_tsb <- yrs2007_2009 %>% 
  select(-date, -time) %>%
  pivot_longer(names_to = "submeter",
               values_to = "reading",
               kitchen:AC) %>%
  as_tsibble(key = submeter,
            index = date_time,
            validate = TRUE)
```


## Missing Values

Data documentation reports 1.25% of observations are missing, and missing values are represented by blanks (gaps). Time series analysis/forcasting requires time interval with no gaps.

```{r}
#counts gaps
yrs_tsb %>%
  count_gaps() 
```

```{r}
#fill implicit missing values with 0
yrs_tsb <- yrs_tsb %>%
  fill_gaps(reading = 0)
```











# EXPLORATION ###########################################

## Daily Analysis

It may be illuminating to zoom in on a single day and examine the pattern of individual readings (instead of averages) from each submeter throught the day. 



```{r, warning=FALSE,  message=FALSE, cache=TRUE}
# plot readings for Jan. 9, 2008
yrs_tsb %>% 
  filter_index("2008-01-09") %>% 
  autoplot(reading) +
  ggtitle("Submeter Readings for January 9, 2008") +
  labs(x = " ", y = "Submeter Reading (watt-hours)")
```

### Reduce Granularity

The dataset contains 1 reading per minute for each submeter and covers a 3 year timespan (aprox. 4.73 million readings). For some visualizations and applications that level of granularity is unnecessary, and it would make sense to use a subset of those readings (e.g. 1 reading from each submeter every 10 min.). That can be accomplished by filtering the data. 

```{r}
# get avg. readings on jan 9, 2008 using complete dataset
yrs_tsb %>% 
  index_by(janday = as_date('20080109')) %>% 
  group_by_key() %>%
  summarise(avg_reading = mean(reading))
```


```{r}
# get avg. readings on Jan 9, 2008 using subset 
yrs_tsb %>% 
  filter(minute(date_time) %in% c(0, 10, 20, 30, 40, 50)) %>% 
  index_by(janday = as_date('20080109')) %>% 
  group_by_key() %>%
  summarise(avg_reading = mean(reading))
```

>*Notes*
>
- The avg. reading for each submeter on January 8, 2009 are nearly identical when calculated using the full dataset vs. a subset containing only 1 reading every 10 min. So for most applications we can be confident that results obtained when using a subset of the data will stil be reliable. 

```{r, message=FALSE, cache=TRUE}
yrs_tsb %>% 
  filter(minute(date_time) %in% c(0, 10, 20, 30, 40, 50)) %>% 
  filter_index("2008-01-09") %>% 
  autoplot(reading)+
  labs(title = "Submeter Readings for January 9, 2008",
       x = "",
       y = "Reading (Watt/Hour)")
  
```

>*Notes*
>
- The plot above was generated from readings at 10 min. intervals.
- Given that we are only looking at a single day, reducing the granularity of the dataset seems unnecessary and does not reveal any additional information. 




## Weekly Analysis

```{r}
# plot weekly avg readings
yrs_tsb %>% 
  group_by_key() %>% 
  index_by(yearweek = yearweek(date_time)) %>% 
  summarize(avg_reading = mean(reading)) %>% 
  autoplot(avg_reading) +
  ggtitle("Weekly Average Reading")+
  labs(x = " ", y = "Submeter Reading (watt-hour)")
```

>*Notes*
>
- Not surprisingly, the submeter monitoring energy usage for the AC and water heater in the home consistently has a much higher average reading than the other two submeters—indicating that the AC and water heater units account for a significant proportion of the total energy usage in the home.  
- Appears to be some seasonality in the energy usage, esp. for the submeter monitoring AC and water heater usage. But the data is a bit too noisey. Let's look at the monthly average readings to see if we can get more insight. 



### Week of April 7-13, 2008

Let's look at the energy usage over the first full week (Mon - Sun) in April 2008. ![2008 Calendar](partial_2008_cal.png) Since the full dataset contains over 30,000 observations for that time period, using all the data points would make the graph too crowded--so it make sense to use the 10 min subset here.

```{r, cache=TRUE, message=FALSE}
# plot submeter readings for April 9-13, 2008 using subset
yrs_tsb %>%
  filter(minute(date_time) %in% c(0, 10, 20, 30, 40, 50)) %>%
  filter_index("2008-04-07" ~ "2008-04-13") %>% 
  autoplot(reading) +
  ggtitle("Submeter Readings for April 7-13, 2008")+
  labs(x = " ", y = "Submeter Reading" )
```







## Monthly Analysis 


```{r, cache=TRUE}
# plot monthly avg reading 
yrs_tsb %>%
  group_by_key() %>% 
  index_by(year_month = yearmonth(date_time)) %>% 
  summarize(avg_reading = mean(reading)) %>% 
  ggplot(aes(x = year_month,
             y = avg_reading,
             color = submeter))+
  geom_line() +
  labs(title = "Monthly Average Submeter Readings (2007-2009)",
       x = " ",
       y = "Average Reading (watt-hour)")
```

>*Notes*
>
- The plot of monthly average readings has the same basic pattern we saw with the plot of weekly average readings. 
- But this plot shows the seasonaility more clearly, and also appears to show an unusual dip in energy usage in all three submeters sometime in the sum




## Nonstandard Periods

In addition to vizualising the data in regular periods (e.g. days, weeks, months, years), it may also be appropriate to look at non-standard intervals. The next plot is of the readings for submeter 3 (AC and water heater) only on Mondays at 8PM from 2007-2009.

```{r, cache=TRUE}
# plot readings for mondays at 8pm
yrs_tsb %>% 
  filter(weekdays(date_time) %in% "Monday") %>% 
  filter(hour(date_time) %in% 20) %>%
  filter(minute(date_time) %in% 01) %>% 
  filter(submeter == "AC") %>% 
  autoplot(reading) +
  labs(title = 'AC Submeter Readings for Mondays at 8PM',
       x = "", y = 'Reading (watt-hours)')
```



## Seasonality Analysis

```{r}
yrs_tsb %>% 
  group_by_key() %>% 
  index_by(day = ~lubridate::as_date(.)) %>% 
  gg_season(reading, period = "month")
```


### Daily Average 

```{r}
yrs_tsb %>% 
  filter(year(date_time) == '2007') %>%
  group_by_key() %>% 
  index_by(day = ~lubridate::as_date(.)) %>% 
  summarise(day_avg = mean(reading)) %>%
  ungroup() %>% 
  gg_season(day_avg)+
  ggtitle("2007 Daily Average Reading")
```


```{r}
# seasonal plot for 2008
yrs_tsb %>% 
  filter(year(date_time) == '2008') %>%
  group_by_key() %>% 
  index_by(day = ~lubridate::as_date(.)) %>% 
  summarise(day_avg = mean(reading)) %>%
  ungroup() %>% 
  gg_season(day_avg) +
  ggtitle("2008 Daily Average Reading")
```

```{r}
# seasonal plot for 2009 
yrs_tsb %>% 
  filter(year(date_time) == '2009') %>%
  group_by_key() %>% 
  index_by(day = ~lubridate::as_date(.)) %>% 
  summarise(day_avg = mean(reading)) %>%
  gg_season(day_avg) +
  ggtitle("2009 Daily Average Reading")
```

>*Notes*
>
- In the daily average seasonality plots for 2008, there is a distict and prolonged trough in energy usage which appears to be sometime August or September. That could indicate a span of time when there was no one occupying the house. 
- Comparing 2008 to 2007 and 2009, the trough in 2008 is, indeed, unusual.
- There also seem to be some monthly and seasonality trends in energy use in all three years. 




### Monthly Average

```{r, cache=TRUE}
# plot monthly avg. reading over time
yrs_tsb %>%
  index_by(yearmonth(date_time)) %>% 
  summarize(avg_reading = mean(reading))%>% 
  gg_season(avg_reading)
```

>*Notes*
>
- Looking at a seasonal plot of monthly averages, it is apparent that August 2008 had the lowest average AC usage of any month during the three year timespan. 
- However, it should be noted that the August 2008 average is perhaps only 0.5 watt-hours below the July average in 2009.






# FORECASTING #############################################




## Create forcast for submeter 3 (AC), 20 time periods ahead
```{r}
# create tsibble
s3_1x_wk <-  yrs2007_2009 %>% 
  filter(weekdays(date_time) %in% "Monday") %>% 
  filter(hour(date_time) %in% 20) %>%
  filter(minute(date_time) %in% 01) %>% 
  pivot_longer(names_to = "submeter",
               values_to = "reading",
               kitchen:AC) %>% 
  select(date, submeter, reading) %>% 
  filter(submeter == "AC") %>% 
  mutate(date = ymd(date)) %>% 
  as_tsibble(key = submeter, index = date, validate = TRUE)
```

```{r}
s3_1x_wk %>% autoplot(reading)
```

```{r}
fit_s3 <-  s3_1x_wk %>% 
  model(TSLM(reading ~ trend()))

#create forecast
fit_s3 %>% 
  forecast(h = "20 weeks") %>% 
  autoplot(s3_1x_wk)
```


```{r}
#fit seasonal naive model
fit_s301 <-  s3_1x_wk %>% 
  model(SNAIVE(reading ~ lag(7)))

#create forecast and plot
fit_s301 %>% 
  forecast(h = "20 weeks") %>% 
  autoplot(s3_1x_wk)
```

```{r}
# fit stl decomp model
my_dcmp_spec <- decomposition_model(
  STL(reading),
  ETS(season_adjust ~ season("N"))
)

#create forecast and plot
s3_1x_wk %>%
  model(stl_ets = my_dcmp_spec) %>%
  forecast(h = "20 weeks") %>%
  autoplot(s3_1x_wk) 

```



## Faorecast Weekly Averages for 2008

```{r}
# create tsibble of daily avgs for 2008
daily_avg_08 <- yrs_tsb %>% 
  filter(year(date_time) == '2008') %>%
  group_by_key() %>% 
  index_by(day = ~lubridate::as_date(.)) %>% 
  summarise(day_avg = mean(reading)) %>% 
  as_tsibble(key = submeter, index = day)
```

```{r}
fit_daily08 <-  daily_avg_08 %>% 
  model(ETS(day_avg))

#create forecast and plot
fit_daily08 %>% 
  forecast(h = "10 weeks") %>% 
  autoplot(daily_avg_08)
```

## Create Additional Forecast

```{r}
 yrs2007_2009 %>% 
  filter(weekdays(date_time) %in% c("Saturday", "Sunday")) %>% 
  filter(hour(date_time) %in% c(0, 10, 20)) %>%
  filter(minute(date_time) %in% 01) %>% 
  pivot_longer(names_to = "submeter",
               values_to = "reading",
               kitchen:AC) %>% 
  select(date, submeter, reading) %>% 
  filter(submeter == "AC") %>% 
  mutate(date = ymd(date)) %>% 
  as_tsibble(key = submeter, index = date, validate = TRUE)
```




# DECOMPOSITION ########################################



# HOLT-WINTERS #########################################


# SUMMARY ###############################################










